{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d182de60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Collecting wandb~=0.15.5\n",
      "  Using cached wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
      "Collecting numpy~=1.24.3\n",
      "  Downloading numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 776 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision==0.15.2 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.15.2)\n",
      "Collecting opencv-python~=4.8.0.74\n",
      "  Using cached opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "Collecting requests~=2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting setuptools~=47.1.0\n",
      "  Downloading setuptools-47.1.1-py3-none-any.whl (583 kB)\n",
      "\u001b[K     |████████████████████████████████| 583 kB 70.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.1.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (10.2.10.91)\n",
      "Requirement already satisfied: filelock in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.7.91)\n",
      "Requirement already satisfied: networkx in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (10.9.0.58)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from torchvision==0.15.2->-r requirements.txt (line 4)) (9.0.1)\n",
      "Requirement already satisfied: wheel in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (16.0.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from wandb~=0.15.5->-r requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: PyYAML in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from wandb~=0.15.5->-r requirements.txt (line 2)) (6.0)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "\u001b[K     |████████████████████████████████| 188 kB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from wandb~=0.15.5->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from wandb~=0.15.5->-r requirements.txt (line 2)) (3.19.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from wandb~=0.15.5->-r requirements.txt (line 2)) (5.8.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.28.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[K     |████████████████████████████████| 213 kB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from requests~=2.31.0->-r requirements.txt (line 6)) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from requests~=2.31.0->-r requirements.txt (line 6)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from requests~=2.31.0->-r requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from requests~=2.31.0->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb~=0.15.5->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/zhiyi/anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.2.1)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=b7b5436471c72493040e95dde17df0f076abc46451bc4c5cf620e75eb5cb1b5e\n",
      "  Stored in directory: /home/zhiyi/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built pathtools\n",
      "Installing collected packages: setuptools, smmap, urllib3, gitdb, setproctitle, sentry-sdk, requests, pathtools, numpy, GitPython, docker-pycreds, wandb, opencv-python\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 61.2.0\n",
      "    Uninstalling setuptools-61.2.0:\n",
      "      Successfully uninstalled setuptools-61.2.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "spyder 5.1.5 requires setuptools>=49.6.0, but you have setuptools 47.1.1 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.4 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.4 which is incompatible.\n",
      "botocore 1.29.76 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.0.3 which is incompatible.\u001b[0m\n",
      "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 numpy-1.24.4 opencv-python-4.8.0.74 pathtools-0.1.2 requests-2.31.0 sentry-sdk-1.28.0 setproctitle-1.3.2 setuptools-47.1.1 smmap-5.0.0 urllib3-2.0.3 wandb-0.15.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c252c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3abb1e",
   "metadata": {},
   "source": [
    "# Define the AutoEncoder model \n",
    "### Encoder: 2 blocks of conv + relu + pool \n",
    "### Decoder: 2 blocks of unpool + transposed conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 100, 5, padding=2),\n",
    "            act_fn, )\n",
    "        self.pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(100, 200, 5, padding=2),\n",
    "            act_fn,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv_1(x)  # (3, 256, 256) -> (100, 256, 256)\n",
    "        output, indices_1 = self.pool(output)  # (100, 256, 256) -> (100, 128, 128)\n",
    "        output = self.conv_2(output)  # (100, 128, 128) -> (200, 128, 128)\n",
    "        output, indices_2 = self.pool(output)  # (200, 128, 128) -> (200, 64, 64)\n",
    "        return output, indices_1, indices_2\n",
    "\n",
    "\n",
    "#  defining decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(200, 100, 5, padding=2),\n",
    "            act_fn, )\n",
    "        self.deconv_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 3, 5, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, indices_1, indices_2):\n",
    "        output = self.unpool(x, indices_1)  # (200, 64, 64) -> (200, 128, 128)\n",
    "        output = self.deconv_1(output)  # (200, 128, 128) -> (100, 128, 128)\n",
    "        output = self.unpool(output, indices_2) # (100, 128, 128) -> (200, 256, 256)\n",
    "        output = self.deconv_2(output) # (100, 256, 256) -> (3, 256, 256)\n",
    "        return output\n",
    "\n",
    "\n",
    "#  defining autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.to(device)\n",
    "\n",
    "        self.decoder = decoder\n",
    "        self.decoder.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, indices_2, indices_1 = self.encoder(x) # returning the pooling indices as well\n",
    "        decoded = self.decoder(encoded, indices_1, indices_2)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282e104",
   "metadata": {},
   "source": [
    "# Define the final classifier model\n",
    "\n",
    "It first used the pretrained encoder to extract the features of the the image, then use fully connected layers for final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f5272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClf(nn.Module):\n",
    "    def __init__(self, encoder,  device, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():  # freeze the feature extractor\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.encoder.to(device)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(200*64*64, 400),\n",
    "            act_fn,\n",
    "            nn.Linear(400, 200),\n",
    "            act_fn,\n",
    "            nn.Linear(200, 3),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359ff14",
   "metadata": {},
   "source": [
    "# Define the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e10074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, device, preprocess, test=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.test = test\n",
    "        self.device = device\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.image_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, \"image_\" + str(idx) + \".jpg\")\n",
    "        image = read_image(image_path)\n",
    "        image = torch.tensor(image)\n",
    "        image = image[:3, :, :]\n",
    "\n",
    "        # Works with 3 channels\n",
    "        mask = np.random.choice([0, 1], size=(256, 256), p=[.2, .8]).astype(np.uint8)\n",
    "        noise = cv2.bitwise_and(image.permute(1, 2, 0).numpy(), image.permute(1, 2, 0).numpy(), mask=mask)\n",
    "        noise = torch.tensor(noise)\n",
    "        noise = noise.permute(2, 0, 1)\n",
    "        noise = noise[:3, :, :]\n",
    "\n",
    "        image = self.preprocess(image)\n",
    "        noise = self.preprocess(noise)\n",
    "\n",
    "\n",
    "        return noise.to(self.device), image.to(\n",
    "            self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc3b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClfImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, labels, device, preprocess, test=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.test = test\n",
    "        self.device = device\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.image_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, \"image_\" + str(idx) + \".jpg\")\n",
    "        image = read_image(image_path)\n",
    "        image = image[:3, :, :]  # Works with 3 channels\n",
    "        image = self.preprocess(image)\n",
    "\n",
    "        image = torch.tensor(image)\n",
    "        label = torch.tensor(self.labels[idx, :])\n",
    "        return image.to(self.device), label.to(self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382676f9",
   "metadata": {},
   "source": [
    "# Start Training Convolutional AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87a6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, save_path, patience=5, verbose=False, delta=0, save=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.save_path = save_path\n",
    "        self.save = save\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            if self.save:\n",
    "                self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            if self.save:\n",
    "                self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.save_path)\t# save the current best model\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50022ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cae():\n",
    "    image_dir = \"./data/train_cae/image/\"\n",
    "    noise_dir = \"./data/train_cae/noise/\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Preprocess the image to normalize the pixel values\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Randomly select 5500 images and discard the rest\n",
    "    full_dataset = ImageDataset(image_dir, device=device, preprocess=preprocess)\n",
    "    use_size = 5500\n",
    "    rest_size = len(full_dataset) - use_size\n",
    "    use_dataset, _ = torch.utils.data.random_split(full_dataset, [use_size, rest_size])\n",
    "\n",
    "    # Split into train and validation dataset\n",
    "    train_size = 5000\n",
    "    val_size = 500\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(use_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "    model = Autoencoder(Encoder(), Decoder(), device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    n_epoch = 30\n",
    "    step = 0\n",
    "\n",
    "    # Check and specify the checkpoint path\n",
    "    ckpt_path = \"/cluster/scratch/zhiychen/DeepPainter/checkpoint\" if torch.cuda.is_available() else './checkpoint'\n",
    "    os.makedirs(ckpt_path, exist_ok=True)\n",
    "    save_filename = '%s_%s.pth' % (n_epoch, \"cae\")\n",
    "    save_path = os.path.join(ckpt_path, save_filename)\n",
    "    early_stopping = EarlyStopping(save_path=save_path, patience=5, verbose=False, delta=0)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            input, label = data\n",
    "            optimizer.zero_grad()\n",
    "            # temp_1 = input[0, :, :, :].permute(1, 2, 0).detach().numpy()\n",
    "            # cv2.imshow(\"image\", temp_1)\n",
    "            # cv2.waitKey(0)\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "            # temp_2 = output[0, :, :, :].permute(1, 2, 0).detach().numpy()\n",
    "            # cv2.imshow(\"image\", temp_2)\n",
    "            # cv2.waitKey(0)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            train_loss += loss.item()\n",
    "            # logging\n",
    "            print({\"Train_loss\": loss.item()})\n",
    "        print({\"Total_Train_loss\": train_loss})\n",
    "\n",
    "        # Validation after each epoch\n",
    "        total_loss, batch_count = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input, label = batch\n",
    "                output = model(input)\n",
    "                loss = criterion(output, label)\n",
    "                batch_count += len(input)\n",
    "                total_loss += loss.item() * len(input)\n",
    "        valid_loss = total_loss / batch_count\n",
    "        print({\"Valid_loss\": valid_loss, \"Epochs\": epoch})\n",
    "\n",
    "        # for early stopping\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a47734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11205/4100305342.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image)\n"
     ]
    }
   ],
   "source": [
    "train_cae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ef823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
